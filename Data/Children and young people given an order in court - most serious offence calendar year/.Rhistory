install.packages("doParelle")
library(doParelle)
library(boot)
library(foreach)
# parelle processing in not avalible for R version 4.0.1
error_rate = [0:20]
# parelle processing in not avalible for R version 4.0.1
error_rate = [rep0:20]
# parelle processing in not avalible for R version 4.0.1
error_rate = [rep(0:20)]
# parelle processing in not avalible for R version 4.0.1
error_rate = [rep(0m20)]
# parelle processing in not avalible for R version 4.0.1
error_rate = [rep(0,20)]
# parelle processing in not avalible for R version 4.0.1
error_rate = [0:20]
# parelle processing in not avalible for R version 4.0.1
error_rate = list[0:20]
# parelle processing in not avalible for R version 4.0.1
error_rate = rep(20)
error_rate
# parelle processing in not avalible for R version 4.0.1
error_rate = rep(,120)
# parelle processing in not avalible for R version 4.0.1
error_rate = rep(1,20)
error_rate
# parelle processing in not avalible for R version 4.0.1
error_rate = rep(0,20)
print(i);
for(int i = 0 in range(0:nrep)){
for i in range(0:nrep)){
for (i in range(0:nrep)){
print(i);
}
nrep = 20
for (i in range(0:nrep)){
print(i);
}
for (i in range(0:20)){
print(i);
}
for (i in 0:nrep){
print(i);
}
nrep = 19
for (i in 0:nrep){
print(i);
}
cv.error <- rep(0,20)
cv.error
k = 10
rep = 20
version
install.packages("doParallel")
library(doParallel)
library(doParallel)
nlust <- makeCluster(detectCores()*0.75)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
library(boot)
library(foreach)
library(doParallel)
varible.indicies <- c(10, 0, 1, 4, 14)
all.comb <- expand.grid(as.data.frame(matrix(rep(0:1, length(varible.indicies)), nrow = 2)))[-1,]
nrep <- 20
set.seed(1)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
# Total error rate
error.rate.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~", paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data
= hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = total.error.rate, K =
10)$delta[1])
}
varible.indicies <- c(10, 0, 1, 4, 14)
variable.indicies <- c(10, 0, 1, 4, 14)
library(boot)
library(foreach)
library(doParallel)
variable.indicies <- c(10, 0, 1, 4, 14)
all.comb <- expand.grid(as.data.frame(matrix(rep(0:1, length(varible.indicies)), nrow = 2)))[-1,]
nrep <- 20
set.seed(1)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
# Total error rate
error.rate.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~", paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data
= hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = total.error.rate, K =
10)$delta[1])
}
library(boot)
library(foreach)
library(doParallel)
variable.indices <- c(10, 0, 1, 4, 14)
all.comb <- expand.grid(as.data.frame(matrix(rep(0:1, length(varible.indicies)), nrow = 2)))[-1,]
nrep <- 20
set.seed(1)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
# Total error rate
error.rate.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~", paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data
= hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = total.error.rate, K =
10)$delta[1])
}
variable.indices <- c(10, 1, 2, 4, 14)
library(boot)
library(foreach)
library(doParallel)
variable.indices <- c(10, 1, 2, 4, 14)
all.comb <- expand.grid(as.data.frame(matrix(rep(0:1, length(varible.indicies)), nrow = 2)))[-1,]
nrep <- 20
set.seed(1)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
# Total error rate
error.rate.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~", paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data = hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = total.error.rate, K = 10)$delta[1])
}
# Area under the curve
variable.indices <- c(11, 1, 2, 5, 15)
library(boot)
library(foreach)
library(doParallel)
variable.indices <- c(11, 1, 2, 5, 15)
all.comb <- expand.grid(as.data.frame(matrix(rep(0:1, length(varible.indicies)), nrow = 2)))[-1,]
nrep <- 20
set.seed(1)
nclust <- makeCluster(detectCores()*0.75)
registerDoParallel(nclust)
# Total error rate
error.rate.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~", paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data = hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = total.error.rate, K = 10)$delta[1])
}
# Area under the curve
# Area under the curve
AUC.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("SURVIVED ~",
paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data = hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = area.under.curve, K =10)$delta[1])
}
# Area under the curve
AUC.parallel <- foreach(i = 1 : nrep, .combine = "rbind", .packages = "boot") %:%
foreach(j = 1 : nrow(all.comb), .combine = "c") %dopar%
{
logistic.regression.model <- glm(as.formula(paste("HD_RISK ~",
paste(names(hd.complete)[variable.indices[all.comb[j,] == 1]], collapse = " + "))), data = hd.complete, family = "binomial")
return(cv.glm(hd.complete, logistic.regression.model, cost = area.under.curve, K =10)$delta[1])
}
# Shut down cores.
stopCluster(nclust)
for (in in 1: length(best.models.error.rate)){
for (i in 1: length(best.models.error.rate)){
cat(paste("Model", i, ":\n"))
print(names(hd.complete)[variable.indices[all.comb[best.models.error.rate[i], ] == 1]])
# Variable names
print(apply(error.rate.parallel, 2, mean)[best.models.error.rate[i]]) # Error rate
cat("\n")
}
# View all models within one SE of the best model.
best.models.error.rate <- (1 : nrow(all.comb))[apply(error.rate.parallel, 2, mean) <=
min(apply(error.rate.parallel, 2, mean) + apply(error.rate.parallel, 2, sd))]
for (i in 1: length(best.models.error.rate)){
cat(paste("Model", i, ":\n"))
print(names(hd.complete)[variable.indices[all.comb[best.models.error.rate[i], ] == 1]])
# Variable names
print(apply(error.rate.parallel, 2, mean)[best.models.error.rate[i]]) # Error rate
cat("\n")
}
min(error.rate.parallel)
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod])
best.mod2 <- max(AUC.parallel)
best.mod2 <- max(AUC.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod2, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod2])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]]])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ],]])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]==2]])
print(apply(error.rate.parallel, 2, mean)[best.mod])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod])
best.mod2 <- max(AUC.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod2, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod2])
best.mod2
best.mod
best.mod2 <- max(apply(AUC.parallel,2, mean))
best.mod2 <- max(apply(AUC.parallel,2, mean))
print(names(hd.complete)[variable.indices[all.comb[best.mod2, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod2])
best.mod2
best.mod2 <- max(apply(AUC.parallel,2, mean))
best.mod2
best.mod2 <- max(AUC.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod2, ]==1]])
print(apply(AUC.parallel, 2, mean)[best.mod2])
best.mod <- min(error.rate.parallel)
print(names(hd.complete)[variable.indices[all.comb[best.mod, ]==1]])
print(apply(error.rate.parallel, 2, mean)[best.mod])
max(AUC.parallel)
backward.selec <- stepAIC(glm(HD_RISK ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, data = hd.complete, family = "binomial"),
scope = list(upper = ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC,
lower = ~ 1), direction = 'backward', trace = FALSE)
library(MASS)
backward.selec <- stepAIC(glm(HD_RISK ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, data = hd.complete, family = "binomial"),
scope = list(upper = ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC,
lower = ~ 1), direction = 'backward', trace = FALSE)
pander(backward.selec$anova)
backward.selec <- stepAIC(glm(HD_RISK ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, data = hd.complete, family = "binomial"), scope = list(upper = ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, lower = ~ 1), direction = 'backward', trace = FALSE)
pander(backward.selec$anova)
library(pander)
library(pander)
backward.selec <- stepAIC(glm(HD_RISK ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, data = hd.complete, family = "binomial"), scope = list(upper = ~ SBP + DBP + factor(SEX) + AGE + factor(EDUC) + CIG + CHOL + BMI + GLUC, lower = ~ 1), direction = 'backward', trace = FALSE)
pander(backward.selec$anova)
tinytex::reinstall_tinytex()
tinytex:::is_tinytex()
b = rbind(c(1,2,4), c(2,3,7))
det(b)
b
det(b)
rank(b)
qr(B)$rank
qr(b)$rank
rnorm(50, mean = 50, sd = 4)
# option 1
rnorm(50, mean = 50, sd = 4)
# option 1
y <-rnorm(50, mean = 50, sd = 4)
var <- var(y)
mean <- mean(y)
mle <- log_liklihood(mean, var)
# define the log-liklihood function
log_liklihood <- function(mu, var){
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
mle <- log_liklihood(mean, var)
n=50
# define the log-liklihood function
log_liklihood <- function(mu, var, n){
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
mle <- log_liklihood(mean, var, n)
# define the log-liklihood function
log_liklihood <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = log_liklihood, par = pars.start, lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B", control = list(fnscle=-1), hessian=T)
# define the log-liklihood function
log_liklihood <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = log_liklihood, par = pars.start, lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B", control = list(fnscle=-1), hessian=T)
set.seed(12)
# option 1
y <-rnorm(50, mean = 50, sd = 4)
mean <- mean(y)
var <- var(y)
n=50
var <- var(y)
# define the log-liklihood function
log_liklihood <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = log_liklihood, par = pars.start, lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B", control = list(fnscle=-1), hessian=T)
norm.fit <- optim(fn = loglik, par = pars.start, lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B", control = list(fnscle=-1), hessian=T)
var <- para[2]
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start, lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B", control = list(fnscle=-1), hessian=T)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
norm.fit
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
sample2 <- rnorm(50)*50
sample1 <-rnorm(50, mean = 50, sd = 2)
x <- rnorm(1, mean=50, sd=2)
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3 <- rnorm(1, mean=50, sd=2)
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3
sample3 <- rnorm(50, mean=50, sd=2)
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3
sample3 <- [0:50]
sample3 <- 0:50
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3
sammple3 <- rbind(rnorm(1, mean=50, sd=2), sample3)
sample3 <- rnorm(1, mean=50, sd=2)
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), sample3)
}
sample3
sample3 <- rnorm(50, mean=50, sd=2)
for(i in 1:length(sample3)){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), sample3)
}
sample3
x <- rnorm(1, mean=50, sd=2)
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
x <- rnorm(1, mean=50, sd=2)
for(i in 1:50){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3
set.seed(12)
sample1 <-rnorm(50, mean = 50, sd = 2)
#mean <- mean(y)
#var <- var(y)
sample2 <- rnorm(50)*50
x <- rnorm(1, mean=50, sd=2)
for(i in 1:50){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
sample3
set.seed(12)
sample1 <-rnorm(50, mean = 50, sd = 2)
#mean <- mean(y)
#var <- var(y)
sample2 <- rnorm(50)*50
x <- rnorm(1, mean=50, sd=2)
for(i in 1:50){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
set.seed(12)
sample1 <-rnorm(50, mean = 50, sd = 2)
sample2 <- rnorm(50)*50
x <- rnorm(1, mean=50, sd=2)
for(i in 1:50){
sammple3 <- rbind(rnorm(1, mean=50, sd=2), x)
}
e = rnorm(length(sample2), 0,2)
x = sample2
e = rnorm(length(sample2), 0,2)
y = x+c
y = x+e
df = fata.frame(x,y)
df = data.frame(x,y)
mod1 <- lm(y~x)
X = model.matrix(mod1)
bHat <- solve(t(X) %*% X) %*% t(X) %*% y
yHat <- X%*%bHat
SSR <- t(y-yHat) %*% (y-yHat)
SSE <- t(y-yHat) %*% (y-yHat)
sigmaSq <- SSE/(length(x)-2)
# Least square estimates
LSE <- c(mu.hat=mean(y), sigmasq.hat= sigmaSq)
LSE
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
b <- rbind(c(1,2,4), c(2,3,7))
qr(b)$rank
n=50
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
n <- 50
y <- sample1
# define the log-liklihood function
loglik <- function(para){
mu <- para[1]
var <- para[2]
return(n*log(1/sqrt(2*pi)) - n/2*log(var) - 1/(2*var)*sum((y-mu)^2))
}
pars.start<- c(50,2)
norm.fit <- optim(fn = loglik, par = pars.start,
lower = c(0,0), upper = c(Inf, Inf),
method = "L-BFGS-B",
control = list(fnscale=-1), hessian=T)
norm.fit
mle <- c(mu.hat=norm.fit$par[1], sigmasq.hat= norm.fit$par[2])
mle
setwd('C:\Users\abbey\OneDrive\UniWork_2020\DATA301\New folder\DATA301-Group-Project\Data')
setwd("C:\Users\abbey\OneDrive\UniWork_2020\DATA301\New folder\DATA301-Group-Project\Data")
setwd("\Users\abbey\OneDrive\UniWork_2020\DATA301\New folder\DATA301-Group-Project\Data")
setwd("\DATA301-Group-Project\Data")
setwd("DATA301-Group-Project\Data")
setwd("DATA301-Group-Project\Data")
setwd(DATA301-Group-Project\Data)
setwd('DATA301-Group-Project\Data')
setwd("C:/Users/abbey/OneDrive/UniWork_2020/DATA301/New folder/DATA301-Group-Project/Data/Children and young people given an order in court - most serious offence calendar year")
data <- read.csv('TABLECODE7362_Data_7884db4c-0aa7-4ed1-ad06-d542d371e397.csv')
data.head()
data.head
head(data)
unique(age)
unique(Age)
unique(data$Age)
head(data)
